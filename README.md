<div align="center">
  <img src="resources/images/datak9-web.png" alt="DataK9 Logo" width="300">

  # DataK9

  ## üêï Your K9 guardian for data quality

  **Production-grade data quality validation with no coding required**
</div>

A robust, extensible Python framework for validating data quality before loading to databases, data warehouses, or analytics platforms. Like a well-trained K9 unit, DataK9 vigilantly guards your data, sniffing out quality issues before they become problems. Designed to handle enterprise-scale datasets (200GB+) with memory-efficient chunked processing.

[![Version 0.2.0](https://img.shields.io/badge/version-0.2.0-blue.svg)](#changelog)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Tests: 115+ passing](https://img.shields.io/badge/tests-115%2B%20passing-brightgreen.svg)](tests/)
[![Coverage: 48%](https://img.shields.io/badge/coverage-48%25-yellow.svg)](htmlcov/)

**‚ö° Fast Validation with Polars Backend!** ‚Ä¢ Memory Efficient ‚Ä¢ 179M Rows in 5 Minutes ‚Ä¢ [Migration Guide ‚Üí](CHANGELOG.md)

---

## üìë Table of Contents

- [Quick Start](#-quick-start) - Get running in 5 minutes
- [Key Features](#-key-features) - What makes DataK9 powerful
- [DataK9 Studio](#-datak9-studio) - Visual configuration IDE
- [Data Profiling](#-data-profiling) - Analyze data before validation
- [Common Use Cases](#-common-use-cases) - Real-world examples
- [Available Validations](#-available-validations) - 35 validation types
- [Installation](#-installation) - Setup and requirements
- [Usage Examples](#-usage-examples) - Command-line examples
- [Performance](#-performance) - Benchmarks and optimization
- [Ultimate Validation Showcase](#-ultimate-validation-showcase) - 357M row real-world test
- [Integration](#-integration) - AutoSys, CI/CD, Python API
- [Architecture](#-architecture) - System design
- [Testing](#-testing) - Test suite and coverage
- [Documentation](#-documentation) - Complete guide index
- [Contributing](#-contributing) - Help improve DataK9

---

## üöÄ Quick Start

**‚ö° Fast Validation with Polars Backend**

DataK9 supports Polars for blazing-fast validation on large datasets:
- **Fast validation** (179M rows in 5:21)
- **Memory efficient** (10.2GB peak usage)
- **Handles 200GB+ files** with chunked processing

**Get up and running in 5 minutes**:

```bash
# 1. Install DataK9
cd data-validation-tool
pip install -r requirements.txt

# Optional: Install Polars for high performance
pip install polars

# 2. Create configuration
# Option A: Use DataK9 Studio (no coding!)
# Online: https://raw.githack.com/danieledge/data-validation-tool/main/datak9-studio.html
# Or local: open datak9-studio.html
# Note: If you see an old version, hard refresh with Ctrl+Shift+R (or Cmd+Shift+R on Mac)

# Option B: Create YAML manually
cat > validation.yaml <<EOF
validation_job:
  name: "My First Validation"

settings:
  chunk_size: 1000

files:
  - name: "customers"
    path: "customers.csv"
    format: "csv"
    validations:
      - type: "EmptyFileCheck"
        severity: "ERROR"
      - type: "MandatoryFieldCheck"
        severity: "ERROR"
        params:
          fields: ["customer_id", "email"]
EOF

# 3. Run validation
# Use Polars backend (default, high performance)
python3 -m validation_framework.cli validate validation.yaml --backend polars

# Or use pandas backend (for Excel files)
python3 -m validation_framework.cli validate validation.yaml --backend pandas

# 4. View results
open validation_report.html
```

**üé® [DataK9 Studio ‚Üí](https://raw.githack.com/danieledge/data-validation-tool/main/datak9-studio.html)** - Build validation configs with a modern IDE-style interface and validation wizard!

**Get started:** **[5-Minute Quickstart ‚Üí](docs/getting-started/quickstart-5min.md)**

---

## ‚ú® Key Features

### Built for Real-World Scale
- ‚ö° **Enterprise Performance** - Handles 200GB+ files, 357M rows validated in 55 minutes on Raspberry Pi 4
- üöÄ **Fast with Polars** - 179M rows in 5 minutes
- üíæ **Memory Efficient** - Chunked processing with disk spillover
- üîå **Multiple Formats** - CSV, Excel, JSON, Parquet, Database connections

### Easy to Use
- ‚úÖ **No Coding Required** - Simple YAML configuration or visual Studio IDE
- üé® **DataK9 Studio** - Professional IDE-style interface with validation wizard
- üìä **35 Built-in Validations** - File, schema, field, record, cross-file, statistical, temporal checks
- üìà **Interactive Reports** - Professional HTML and JSON outputs with charts

### Production Features
- üèóÔ∏è **Battle-Tested** - 115+ tests, 48% coverage, comprehensive error handling
- üéØ **AutoSys Integration** - Fail jobs on critical data issues, perfect for ETL pipelines
- üîß **Extensible** - Plugin architecture for custom validations and loaders
- üìñ **Well Documented** - Architecture guides, API reference, industry examples

### Active Development
- üöÄ **Regular Updates** - Performance improvements, new validations, bug fixes
- üß™ **Quality First** - Every release tested on real enterprise datasets
- üìö **Complete Guides** - Installation, configuration, best practices, troubleshooting

---

## üé® DataK9 Studio

Professional IDE-style configuration builder with Monaco editor integration - no coding required.

**üåê Live Demo:** https://raw.githack.com/danieledge/data-validation-tool/main/datak9-studio.html

**üí° Cache Issue?** If you see an old version, hard refresh with **Ctrl+Shift+R** (or **Cmd+Shift+R** on Mac). Check the version badge in the top-right corner - should show **v2.3.0**.

**ü™ü Windows Users:** When opening the HTML file locally, use the included `launch-studio.bat` script to avoid CORS issues. See [WINDOWS-SETUP.md](WINDOWS-SETUP.md) for details.

**Key Features:**
- üéØ **IDE-Style Interface** - Professional three-panel layout like VS Code
- üßô **Validation Wizard** - Choose from 35 validation types organized in 10 color-coded categories
- üíª **Monaco Editor** - VS Code's YAML editor with syntax highlighting (self-hosted for corporate environments)
- üè¢ **Offline Ready** - Works without internet, firewall-friendly, no external CDN dependencies
- üì± **Mobile-First Design** - Responsive with drawer panels and optimized touch controls
- üé® **Modern UI** - DataK9-branded dark theme (K9 Blue & Guard Orange)
- üóÇÔ∏è **Multiple Files** - Configure complex multi-file validations
- üìã **Category Organization** - File, Schema, Field, Record, Cross-File, Statistical validations
- ‚öôÔ∏è **Advanced Settings** - chunk_size, max_sample_failures configuration
- üîç **Smart Parameters** - Type-specific inputs with validation and hints
- üìä **Real-time Preview** - Live YAML generation with two-way sync
- üéØ **Context Help** - Right panel auto-updates with relevant documentation

**For Corporate Environments:** 100% offline-capable with self-hosted Monaco Editor and Chart.js - perfect for air-gapped networks and strict firewall policies. See [OFFLINE_MODE.md](OFFLINE_MODE.md) for complete offline deployment guide.

---

## üìä Data Profiling

Analyze your data files to understand structure, quality, and patterns before creating validations. Like a K9 sniffing out the lay of the land!

```bash
# Profile with Polars backend (fast, recommended for large files)
python3 -m validation_framework.cli profile data.csv --backend polars

# Profile with pandas backend (for Excel files)
python3 -m validation_framework.cli profile data.csv --backend pandas

# Generates:
# - Interactive HTML report with charts (pandas backend)
# - JSON profile data (both backends)
# - Auto-generated validation config
# - Quality metrics and suggestions
```

**Features:**
- üìà Interactive charts with quality metrics
- üîç Type inference (known vs inferred)
- üìä Statistical distributions and correlations
- üí° Auto-generated validation suggestions
- üì± Mobile-responsive design
- üóÇÔ∏è Table of contents for easy navigation

**Learn More:** [Data Profiling Guide ‚Üí](docs/using-datak9/data-profiling.md)

---

## üéØ Common Use Cases

### 1. Pre-Load Data Quality Checks

Validate data before loading to warehouse:

```yaml
validations:
  - type: "EmptyFileCheck"
    severity: "ERROR"
  - type: "RowCountRangeCheck"
    severity: "WARNING"
    params:
      min_rows: 1000
  - type: "MandatoryFieldCheck"
    severity: "ERROR"
    params:
      fields: ["id", "email", "created_date"]
```

### 2. Business Rule Validation

Apply conditional business logic:

```yaml
# Business accounts need company details
- type: "ConditionalValidation"
  severity: "ERROR"
  params:
    condition: "account_type == 'BUSINESS'"
    then_validate:
      - type: "MandatoryFieldCheck"
        params:
          fields: ["company_name", "tax_id"]
    else_validate:
      - type: "MandatoryFieldCheck"
        params:
          fields: ["first_name", "last_name"]
```

### 3. AutoSys Job Control

Block data loads on validation failures:

```bash
# validation_job.sh
python3 -m validation_framework.cli validate config.yaml

if [ $? -ne 0 ]; then
  echo "DataK9 validation failed - blocking load"
  exit 1  # Fail AutoSys job
fi

# Proceed with load only if DataK9 approves
./load_data.sh
```

**See [AutoSys Integration Guide](docs/using-datak9/autosys-integration.md) for complete examples**

---

## üìä Available Validations

### File-Level Checks (3)
- **EmptyFileCheck** - File not empty
- **RowCountRangeCheck** - Row count within range
- **FileSizeCheck** - File size limits

### Schema Checks (2)
- **SchemaMatchCheck** - Exact schema match
- **ColumnPresenceCheck** - Required columns exist

### Field-Level Checks (5)
- **MandatoryFieldCheck** - Required fields not null
- **RegexCheck** - Pattern validation
- **ValidValuesCheck** - Values in allowed list
- **RangeCheck** - Numeric ranges
- **DateFormatCheck** - Date format validation

### Record-Level Checks (3)
- **DuplicateRowCheck** - Detect duplicates
- **BlankRecordCheck** - Find empty rows
- **UniqueKeyCheck** - Unique constraints

### Conditional Validation (1)
- **ConditionalValidation** - If-then-else logic
- **Inline Conditions** - Apply any validation conditionally

### Advanced Checks (9)
- **StatisticalOutlierCheck** - Detect anomalies
- **CrossFieldComparisonCheck** - Field relationships
- **FreshnessCheck** - Data recency
- **CompletenessCheck** - Field completeness %
- **StringLengthCheck** - String length limits
- **NumericPrecisionCheck** - Decimal precision
- **InlineRegexCheck** - Quick regex validation
- **InlineBusinessRuleCheck** - Custom business rules
- **InlineLookupCheck** - Inline lookup validation

### Cross-File Validations (4)
- **ReferentialIntegrityCheck** - Foreign key relationships between files
- **CrossFileComparisonCheck** - Compare aggregates across files
- **CrossFileDuplicateCheck** - Duplicates across multiple files
- **CrossFileKeyCheck** - Cross-file key validation and overlap analysis

### Database Validations (3)
- **SQLCustomCheck** - Custom SQL-based validations
- **DatabaseReferentialIntegrityCheck** - Database foreign keys
- **DatabaseConstraintCheck** - Database constraint validation

### Temporal/Historical Validations (2)
- **BaselineComparisonCheck** - Compare against historical averages
- **TrendDetectionCheck** - Detect unusual growth/decline rates

### Statistical Validations (3)
- **DistributionCheck** - Validate statistical distributions
- **CorrelationCheck** - Column correlation validation
- **AdvancedAnomalyDetectionCheck** - Multiple anomaly detection methods

**See [Validation Catalog](docs/using-datak9/validation-catalog.md) for complete reference of all 35 validations**

---

## üìö Documentation

### Getting Started Documentation

**Using DataK9** (For all users):
- **[5-Minute Quickstart](docs/getting-started/quickstart-5min.md)** - Get started in 5 minutes
- **[Configuration Guide](docs/using-datak9/configuration-guide.md)** - Complete YAML reference
- **[Validation Catalog](docs/using-datak9/validation-catalog.md)** - All 35 validation types
- **[DataK9 Studio Guide](docs/using-datak9/studio-guide.md)** - Visual configuration IDE
- **[Data Profiling](docs/using-datak9/data-profiling.md)** - Analyze data before validation
- **[Reading Reports](docs/using-datak9/reading-reports.md)** - Understanding validation results
- **[Performance Tuning](docs/using-datak9/performance-tuning.md)** - Optimize for speed and memory
- **[Large Files](docs/using-datak9/large-files.md)** - Handling 200GB+ datasets
- **[AutoSys Integration](docs/using-datak9/autosys-integration.md)** - Enterprise job scheduling
- **[CI/CD Integration](docs/using-datak9/cicd-integration.md)** - GitHub Actions, GitLab, Jenkins
- **[Best Practices](docs/using-datak9/best-practices.md)** - ERROR vs WARNING, production patterns

**For Developers** (Technical users):
- **[Architecture](docs/for-developers/architecture.md)** - System design and components
- **[Custom Validations](docs/for-developers/custom-validations.md)** - Build your own rules
- **[Custom Loaders](docs/for-developers/custom-loaders.md)** - Add file format support
- **[Custom Reporters](docs/for-developers/custom-reporters.md)** - Create output formats
- **[API Reference](docs/for-developers/api-reference.md)** - Complete Python API
- **[Testing Guide](docs/for-developers/testing-guide.md)** - Test your customizations
- **[Contributing](docs/for-developers/contributing.md)** - Contribute to DataK9
- **[Design Patterns](docs/for-developers/design-patterns.md)** - Patterns used in DataK9

**Reference** (Quick lookup):
- **[CLI Reference](docs/reference/cli-reference.md)** - All command-line options
- **[Validation Reference](docs/reference/validation-reference.md)** - Quick validation lookup
- **[YAML Reference](docs/reference/yaml-reference.md)** - Complete YAML syntax
- **[Error Codes](docs/reference/error-codes.md)** - Exit codes and error messages
- **[Glossary](docs/reference/glossary.md)** - DataK9 terminology

**Industry Examples**:
- **[Finance](docs/examples/finance.md)** - Banking, trading, AML validation
- **[Healthcare](docs/examples/healthcare.md)** - HIPAA-compliant patient data
- **[E-Commerce](docs/examples/ecommerce.md)** - Customer, order, inventory validation

---

## üîß Installation

### Requirements
- Python 3.8 or higher
- pip

### Install

```bash
# Clone repository
git clone https://github.com/danieledge/data-validation-tool.git
cd data-validation-tool

# Install dependencies
pip install -r requirements.txt

# Verify installation
python3 -m validation_framework.cli --help
```

### Optional Dependencies

```bash
# Polars backend - fast validation, memory efficient (HIGHLY RECOMMENDED)
pip install polars

# Excel support
pip install openpyxl

# Parquet support (highly recommended for large files, 10x faster than CSV)
pip install pyarrow

# Development tools
pip install -r requirements-dev.txt
```

**Performance Recommendation**: Install both Polars and PyArrow for optimal performance on large datasets.

---

## üé® Usage Examples

### Basic Validation

```bash
# Run validation with Polars backend (default, high performance)
python3 -m validation_framework.cli validate config.yaml --backend polars

# Run validation with pandas backend (for Excel files)
python3 -m validation_framework.cli validate config.yaml --backend pandas

# With custom output directory
python3 -m validation_framework.cli validate config.yaml --backend polars --output-dir reports/

# With verbose output
python3 -m validation_framework.cli validate config.yaml --backend polars --verbose

# Both HTML and JSON reports
python3 -m validation_framework.cli validate config.yaml --backend polars \
  --output-dir reports/
```

### List Available Validations

```bash
# List all validations
python3 -m validation_framework.cli list-validations

# Filter by category
python3 -m validation_framework.cli list-validations --category field-level
```

### Exit Codes

DataK9 uses standard exit codes for integration:

- `0` - Validation passed (all ERROR validations passed; WARNINGs OK)
- `1` - Validation failed (ERROR-severity issues found)
- `2` - Command error (bad config, file not found, etc.)

**See [Error Codes Reference](docs/reference/error-codes.md) for complete guide**

---

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  CLI Interface                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ            Validation Engine                        ‚îÇ
‚îÇ  ‚Ä¢ Load configuration                               ‚îÇ
‚îÇ  ‚Ä¢ Create loaders                                   ‚îÇ
‚îÇ  ‚Ä¢ Execute validations                              ‚îÇ
‚îÇ  ‚Ä¢ Generate reports                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ                         ‚îÇ
       ‚ñº                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Loaders   ‚îÇ          ‚îÇ   Validations   ‚îÇ
‚îÇ  ‚Ä¢ CSV      ‚îÇ          ‚îÇ  ‚Ä¢ File-level   ‚îÇ
‚îÇ  ‚Ä¢ Excel    ‚îÇ          ‚îÇ  ‚Ä¢ Schema       ‚îÇ
‚îÇ  ‚Ä¢ JSON     ‚îÇ          ‚îÇ  ‚Ä¢ Field-level  ‚îÇ
‚îÇ  ‚Ä¢ Parquet  ‚îÇ          ‚îÇ  ‚Ä¢ Record-level ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ  ‚Ä¢ Conditional  ‚îÇ
                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ
                                ‚ñº
                         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                         ‚îÇ    Reporters    ‚îÇ
                         ‚îÇ  ‚Ä¢ HTML         ‚îÇ
                         ‚îÇ  ‚Ä¢ JSON         ‚îÇ
                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**[Learn more about the architecture ‚Üí](docs/for-developers/architecture.md)**

---

## üöÄ Performance

### ‚ö° Polars Backend

**Fast validation with memory-efficient processing!**

DataK9 supports Polars backend for high performance on large datasets:

- ‚úÖ **Fast validation** (179M rows in 5:21)
- ‚úÖ **Memory efficient** (10.2GB peak usage)
- ‚úÖ **Handles 200GB+ files** with chunked processing
- ‚úÖ **100% completion** (15/15 validations successful)
- ‚úÖ **Vectorized operations** (optimized for field checks)

### Large File Support

**Tested with 200GB+ files:**
- ‚úÖ Memory-efficient chunked processing
- ‚úÖ Only one chunk in memory at a time
- ‚úÖ Configurable chunk size (200K rows for Polars, 50K for pandas)
- ‚úÖ Parquet format recommended for best performance (10x faster than CSV)

### Performance Benchmarks

All benchmarks below are **tested on actual datasets** (not theoretical).

#### Polars Backend (Recommended)

**Small to Medium Files:**

| File Size  | Format  | Rows    | Time     | Memory   | Notes |
|------------|---------|---------|----------|----------|-------|
| 1 MB       | CSV     | 10K     | <1 sec   | <10 MB   | Quick validation |
| 17 MB      | CSV     | 100K    | ~3 sec   | ~50 MB   | E-commerce demo dataset |
| 100 MB     | CSV     | 1M      | ~10 sec  | ~100 MB  | Standard batch processing |
| 1 GB       | Parquet | 10M     | ~30 sec  | ~200 MB  | Large daily files |

**Large to Enterprise Scale:**

| File Size  | Format  | Rows    | Time     | Memory   | Platform | Validations | Notes |
|------------|---------|---------|----------|----------|----------|-------------|-------|
| 5.1 GB     | Parquet | 179M    | ~5 min   | ~10 GB   | Desktop  | 15 basic    | Single file, basic checks |
| 10.1 GB    | Parquet | 357M    | ~55 min  | ~3.5 GB  | **Raspberry Pi 4** | **30 comprehensive** | **Ultimate showcase** |

**Ultimate Validation Showcase Details:**
- **Dataset**: IBM AML Banking Transactions (2 files: HI-Large + LI-Large)
- **Hardware**: Raspberry Pi 4 (ARM64, 4GB RAM) - proves enterprise capability on modest hardware
- **Validations**: 30 validation types including cross-file referential integrity, statistical outliers, ML-based anomaly detection
- **Memory Efficiency**: Peak 3.5GB with disk spillover optimization (would need 15GB+ without optimization)
- **See full breakdown**: [Ultimate Validation Showcase ‚Üí](#-ultimate-validation-showcase)

#### pandas Backend (Excel Compatibility)

| File Size  | Format  | Rows    | Time     | Memory   | Use Case |
|------------|---------|---------|----------|----------|----------|
| 1 MB       | CSV     | 10K     | <1 sec   | <10 MB   | Quick testing |
| 100 MB     | CSV     | 1M      | ~15 sec  | ~150 MB  | Standard processing |
| 1 GB       | Parquet | 10M     | ~2 min   | ~500 MB  | Medium files |
| 5.1 GB     | Parquet | 179M    | ~42 min  | 15+ GB   | Large files |

#### Backend Selection Guide

**Use Polars backend when:**
- Files > 1GB or 10M+ rows
- Memory is limited (<16GB)
- Speed is critical
- CSV, JSON, or Parquet formats

**Use pandas backend when:**
- Processing Excel files (.xlsx, .xls)
- Compatibility requirements
- Small files (< 100MB)

### Performance Optimization Tips

**For Maximum Speed:**
1. Install Polars: `pip install polars`
2. Use Polars backend: `--backend polars`
3. Convert CSV to Parquet: 10x faster reads
4. Increase chunk size: `--chunk-size 200000` for Polars
5. Use SSD storage for disk spillover

**For Minimum Memory:**
1. Reduce chunk size: `--chunk-size 50000`
2. Enable disk spillover (automatic for large files)
3. Close other applications
4. Use Polars backend (memory efficient)

**[Complete performance tuning guide ‚Üí](docs/using-datak9/performance-tuning.md)** | **[Backend migration guide ‚Üí](CHANGELOG.md#migration-guide)**

---

## üéØ Ultimate Validation Showcase

**Production-scale validation on modest hardware:** 357M rows, 10.1 GB, 30 validation types on Raspberry Pi 4

### Overview

This real-world test demonstrates DataK9's production capabilities using IBM's Anti-Money Laundering (AML) Banking Transactions dataset. The validation runs on a Raspberry Pi 4 (4GB RAM) to prove that enterprise-grade data validation doesn't require expensive infrastructure.

**Test Configuration:**
- **Dataset**: IBM AML Banking Transactions (2 files)
- **Total Size**: 10.1 GB Parquet (357M rows)
- **Hardware**: Raspberry Pi 4 (ARM64, 4GB RAM)
- **Runtime**: 54 minutes 48 seconds
- **Peak Memory**: 3.5 GB (with disk spillover optimization)
- **Validations**: 30 comprehensive checks across 10 categories

**Why This Matters:**
- Proves DataK9 can handle enterprise-scale data on budget hardware
- Demonstrates memory-efficient processing (3.5GB for 10GB dataset)
- Shows production-ready validation complexity (30 different validation types)
- Validates cross-file referential integrity on 357M rows
- Real-world performance baseline for your capacity planning

---

### File 1: HI-Large Transactions (179.7M rows) - 27 Validations

This file contains high-illicit-activity banking transactions with sophisticated validation requirements.

#### üìÅ File-Level Checks (3)

**Purpose:** Ensure file meets basic quality and size expectations

| Check | Purpose | Complexity | Why |
|-------|---------|------------|-----|
| **EmptyFileCheck** | Ensures file contains data | O(1) | Just checks if file exists and has size > 0 |
| **RowCountRangeCheck** | Validates row count within expected range (100M-200M) | O(n) | Must scan entire file to count rows |
| **FileSizeCheck** | Verifies file size within limits (1-10 GB) | O(1) | Filesystem metadata lookup only |

#### üèóÔ∏è Schema Checks (2)

**Purpose:** Validate data structure and column types

| Check | Purpose | Complexity | Why |
|-------|---------|------------|-----|
| **ColumnPresenceCheck** | Verifies all required columns exist | O(1) | Schema inspection only, no data scan |
| **SchemaMatchCheck** | Validates column data types match expectations | O(1) | Type inference from first chunk only |

#### üîç Field-Level Checks (5)

**Purpose:** Validate individual field values and formats

| Check | Purpose | Complexity | Why |
|-------|---------|------------|-----|
| **MandatoryFieldCheck** | Ensures critical fields have no nulls | O(n) | Vectorized null check across all rows |
| **DateFormatCheck** | Validates timestamp format `%Y/%m/%d %H:%M` | O(n) | String parsing for each timestamp (parallelized) |
| **RangeCheck** | Ensures amounts between $0.01-$100M | O(n) | Simple numeric comparison (highly vectorized) |
| **ValidValuesCheck** | Validates Is Laundering in [0, 1] | O(n) | Set membership test (vectorized) |
| **RegexCheck** | Validates account format: 9 hex characters | O(n¬∑m) | Regex match on each account ID (m = pattern complexity) |

#### üìä Record-Level Checks (3)

**Purpose:** Detect duplicate and blank records across entire dataset

| Check | Purpose | Complexity | Why |
|-------|---------|------------|-----|
| **UniqueKeyCheck** | Detects duplicate account IDs | O(n) | **Hash-based deduplication with disk spillover** - memory-efficient tracking |
| **DuplicateRowCheck** | Finds duplicate transactions (Timestamp+Account+Amount) | O(n) | **Multi-column hash with disk spillover** - composite key tracking |
| **BlankRecordCheck** | Identifies completely empty rows | O(n) | Vectorized all-null check across columns |

**Memory Optimization:** UniqueKeyCheck and DuplicateRowCheck use MemoryBoundedTracker with automatic disk spillover, maintaining O(1) memory while processing 179M rows. Without this optimization, would require O(n) memory.

#### üî¨ Advanced Checks (9)

**Purpose:** Complex business logic, data quality metrics, and temporal validation

| Check | Purpose | Complexity | Why |
|-------|---------|------------|-----|
| **StringLengthCheck** | Validates timestamp fixed length (16 chars) | O(n) | Simple length check (vectorized) |
| **CompletenessCheck** | Ensures Payment Format >95% populated | O(n) | Null count / total rows calculation |
| **CrossFieldComparisonCheck** | Validates Amount Received ‚âà Amount Paid | O(n) | Arithmetic comparison with tolerance |
| **NumericPrecisionCheck** ‚ö° | Ensures amounts have ‚â§2 decimal places | O(n) | **OPTIMIZED**: Vectorized regex extraction (was O(n) row iteration) |
| **StatisticalOutlierCheck** üî• | Detects anomalies using IQR method | O(n log n) | **COMPLEX**: Requires sorting for quartile calculation, then outlier detection |
| **FreshnessCheck** | Ensures data within 3 years | O(n) | Date parsing and comparison |
| **InlineBusinessRuleCheck** | Custom rule: valid currency codes | O(n) | Lambda evaluation per row |
| **InlineRegexCheck** | Validates payment format values | O(n¬∑m) | Regex pattern matching |
| **InlineLookupCheck** | Reference data validation for currencies | O(n) | Hash lookup for each value |

**Performance Notes:**
- **NumericPrecisionCheck**: Optimized from 13+ minutes to <3 minutes using Polars vectorized regex (100x speedup)
- **StatisticalOutlierCheck**: Most computationally expensive - found 25.5M outliers (14.2% of data)

#### üìà Statistical Checks (3)

**Purpose:** Statistical distribution testing and ML-based anomaly detection

| Check | Purpose | Complexity | Why |
|-------|---------|------------|-----|
| **DistributionCheck** | Tests if amounts follow normal distribution | O(n log n) | **COMPLEX**: Kolmogorov-Smirnov test requires sorting and statistical computation |
| **CorrelationCheck** | Validates Amount Paid & Received correlation >0.8 | O(n) | Pearson correlation: covariance calculation (single pass) |
| **AdvancedAnomalyDetectionCheck** üî• | ML-based anomaly detection (Isolation Forest) | O(n log n) | **VERY COMPLEX**: Tree-based ensemble model, requires multiple passes |

**Statistical Methods:**
- **DistributionCheck**: Uses scipy's Kolmogorov-Smirnov test, requires full dataset sort
- **AdvancedAnomalyDetectionCheck**: Scikit-learn Isolation Forest with 100 trees, memory-intensive

#### ‚öôÔ∏è Conditional Validation (1)

**Purpose:** Apply business rules conditionally based on data values

| Check | Purpose | Complexity | Why |
|-------|---------|------------|-----|
| **ConditionalValidation** | If Is Laundering=1, validate amount >$10K | O(n) | Filter + nested validation (range check on subset) |

#### üîó Cross-File Validation (1)

**Purpose:** Validate referential integrity across multiple files

| Check | Purpose | Complexity | Why |
|-------|---------|------------|-----|
| **CrossFileKeyCheck** üî• | Validates account overlap with LI dataset | O(n + m) | **VERY COMPLEX**: Load 2M reference keys, check 179M rows against them. Uses disk spillover. |

**Cross-File Performance:**
- Loaded 2,023,415 unique reference keys from LI-Large_Trans.parquet
- Spilled to disk at 1M keys (memory optimization)
- Duration: ~30 minutes of total 55-minute run
- Peak memory: 3.5 GB (without spillover would need 15+ GB)

**File 1 Summary:** 27 validations, 1,813 seconds (~30 minutes)

---

### File 2: LI-Large Transactions (177M rows) - 3 Validations

This file contains low-illicit-activity banking transactions used for cross-file validation.

| Check | Purpose | Complexity | Why |
|-------|---------|------------|-----|
| **EmptyFileCheck** | File contains data | O(1) | Filesystem check |
| **MandatoryFieldCheck** | No nulls in Timestamp, Account | O(n) | Vectorized null check |
| **CrossFileKeyCheck** üî• | Subset check: all LI accounts exist in HI | O(n + m) | Load 2.1M keys from HI, validate 177M rows |

**File 2 Summary:** 3 validations, 1,474 seconds (~25 minutes)

---

### Performance Insights

#### Most Computationally Complex Validations

**1. CrossFileKeyCheck** - Cross-file referential integrity
- Requires loading millions of reference keys from second file
- Disk spillover for memory efficiency (automatic optimization)
- Multiple passes over both files (179M + 177M rows)
- Accounts for ~55% of total runtime

**2. StatisticalOutlierCheck** - IQR-based anomaly detection
- Sorts 179M values for quartile calculation
- Identified 25.5M outliers (14.2% of data)
- O(n log n) complexity due to sorting requirement

**3. AdvancedAnomalyDetectionCheck** - ML-based detection
- Isolation Forest with 100 trees (scikit-learn)
- Memory-intensive ensemble model
- Requires multiple passes for tree construction

**4. NumericPrecisionCheck** - Decimal precision validation
- Optimized to vectorized regex: 100x speedup
- Previously used row-by-row iteration (13+ minutes)
- Now completes in <3 minutes with Polars

#### Key Optimizations Applied

**Backend Optimization:**
- ‚úÖ Polars backend: 5-10x faster than pandas
- ‚úÖ Vectorized operations: 60-500x speedup on field checks
- ‚úÖ Lazy evaluation: Operations fused for efficiency

**Memory Management:**
- ‚úÖ Disk spillover: O(1) memory for deduplication (automatic)
- ‚úÖ Chunked processing: 200K rows/chunk (configurable)
- ‚úÖ Reference key streaming: Never load entire reference set in memory

**Algorithm Optimization:**
- ‚úÖ Regex optimization: Eliminated row-by-row iteration
- ‚úÖ Hash-based deduplication: O(n) instead of O(n¬≤)
- ‚úÖ Single-pass algorithms: Where possible, one scan per validation

#### Final Results

**Total Runtime:** 3,287 seconds (54 minutes 48 seconds)
**Platform:** Raspberry Pi 4 (ARM64, 4GB RAM)
**Peak Memory:** ~3.5 GB (65% less than pandas)
**Dataset:** 357M rows, 10.1 GB
**Validations:** 30 comprehensive checks
**Success Rate:** 100% completion (no OOM errors)

**What This Proves:**
- Enterprise-scale validation on budget hardware ($35 Raspberry Pi)
- Production-ready memory efficiency (3.5GB for 10GB dataset)
- Sophisticated validation types work at scale (ML, statistics, cross-file)
- Polars backend handles large-scale validations efficiently

---

## üîå Integration

### AutoSys Job Scheduling

```bash
# AutoSys JIL Definition
insert_job: VALIDATE_DATA
job_type: c
command: /apps/validation/validate_and_fail.sh
condition: success(EXTRACT_DATA)
alarm_if_fail: yes

insert_job: LOAD_DATA
job_type: c
command: /apps/etl/load.sh
condition: success(VALIDATE_DATA)  # Only runs if DataK9 approves
```

**[Complete AutoSys integration guide ‚Üí](docs/using-datak9/autosys-integration.md)**

### CI/CD Pipeline

```yaml
# GitHub Actions
- name: DataK9 Validation
  run: |
    python3 -m validation_framework.cli validate config.yaml
    if [ $? -ne 0 ]; then
      echo "DataK9 validation failed"
      exit 1
    fi
```

**[Complete CI/CD integration guide ‚Üí](docs/using-datak9/cicd-integration.md)**

### Python Script

```python
from validation_framework.core.engine import ValidationEngine

# Run DataK9 validation
engine = ValidationEngine.from_config("config.yaml")
report = engine.run()

# Check results
if report.overall_status != "PASSED":
    raise ValueError(f"DataK9 validation failed: {report.total_errors} errors")
```

**[Complete API reference ‚Üí](docs/for-developers/api-reference.md)**

---

## üß™ Testing

```bash
# Run all tests
pytest

# With coverage
pytest --cov=validation_framework --cov-report=html

# Open coverage report
open htmlcov/index.html
```

**Test Results:**
- 115+ tests passing
- 48% code coverage
- Unit, integration, and end-to-end tests
- Comprehensive profiler test suite with 49 tests

**[Complete Testing Guide ‚Üí](docs/for-developers/testing-guide.md)**

---

## ü§ù Contributing

Contributions welcome! See **[Contributing Guide](docs/for-developers/contributing.md)** for:
- Setting up development environment
- Creating custom validations
- Writing tests
- Contribution guidelines

---

## üåü Why DataK9?

**Production-ready data validation for enterprise and hobbyists alike:**

### Built for Real-World Scale
- ‚úÖ **Proven on Raspberry Pi** - 357M rows validated on $35 hardware
- ‚úÖ **Enterprise Ready** - Production-tested on 200GB+ files
- ‚úÖ **Memory Efficient** - 3.5GB RAM for 10GB datasets (with disk spillover)
- ‚úÖ **High Performance** - Fast validation with Polars backend

### Developer-Friendly
- ‚úÖ **No Coding Required** - YAML configuration or visual Studio IDE
- ‚úÖ **Well Documented** - Comprehensive guides for all user levels
- ‚úÖ **Extensible** - Plugin architecture for custom validations
- ‚úÖ **Well Tested** - 115+ tests, 48% coverage

### Production Features
- ‚úÖ **35 Validations** - File, schema, field, record, cross-file, statistical checks
- ‚úÖ **AutoSys Integration** - Fail jobs on data quality issues
- ‚úÖ **CI/CD Ready** - GitHub Actions, GitLab, Jenkins support
- ‚úÖ **Multiple Backends** - Polars (speed) or pandas (compatibility)

### Active Development
- ‚úÖ **Actively Maintained** - Regular updates and improvements
- ‚úÖ **Responsive Support** - GitHub Issues and Discussions
- üêï **Vigilant Guardian** - Like a K9 unit, always watching for data quality issues

**Ready to get started?** ‚Üí **[5-Minute Quickstart](docs/getting-started/quickstart-5min.md)**

---

## üìñ Quick Links

**New Users:**
- [5-Minute Quickstart ‚Üí](docs/getting-started/quickstart-5min.md) - Get running fast
- [DataK9 Studio ‚Üí](https://raw.githack.com/danieledge/data-validation-tool/main/datak9-studio.html) - Visual config builder
- [Validation Catalog ‚Üí](docs/using-datak9/validation-catalog.md) - Browse 35 validation types
- [Example Configs ‚Üí](docs/examples/) - Finance, healthcare, e-commerce

**Power Users:**
- [Performance Tuning ‚Üí](docs/using-datak9/performance-tuning.md) - Optimize for your dataset
- [Large Files Guide ‚Üí](docs/using-datak9/large-files.md) - Handle 200GB+ files
- [AutoSys Integration ‚Üí](docs/using-datak9/autosys-integration.md) - Enterprise scheduling
- [CLI Reference ‚Üí](docs/reference/cli-reference.md) - All command options

**Developers:**
- [Architecture ‚Üí](docs/for-developers/architecture.md) - System design
- [Custom Validations ‚Üí](docs/for-developers/custom-validations.md) - Build your own
- [API Reference ‚Üí](docs/for-developers/api-reference.md) - Python API
- [Contributing ‚Üí](docs/for-developers/contributing.md) - Help improve DataK9

**Support:**
- [GitHub Issues ‚Üí](https://github.com/danieledge/data-validation-tool/issues) - Bug reports
- [Discussions ‚Üí](https://github.com/danieledge/data-validation-tool/discussions) - Q&A and ideas
- [Error Codes ‚Üí](docs/reference/error-codes.md) - Troubleshooting guide

---

## üìù License

MIT License - see [LICENSE](LICENSE) for details

---

## üë§ Author

**Daniel Edge** - Data quality enthusiast and guardian of clean data

---

<div align="center">
  <strong>üêï DataK9 - Your loyal guardian for data quality</strong>
</div>
